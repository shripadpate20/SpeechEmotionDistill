# ğŸ§ Speech Emotion Recognition with Knowledge Distillation

This repository implements **Speech Emotion Recognition (SER)** using a compact student CNN model trained via **Knowledge Distillation** from a larger pre-trained **Wav2Vec2.0 teacher model**. The goal is to compress a large model into a lightweight one while maintaining performance â€” ideal for deployment in resource-constrained environments like mobile devices.

## ğŸ”— Live Demo

ğŸ‘‰ [Try the model on Streamlit!](https://speechemotiondistill-sjfvberakplyvwznf62bri.streamlit.app/)

---

## ğŸ§  Key Concepts

- **Teacher Model**: `Wav2Vec2ForSequenceClassification` (from HuggingFace Transformers)
- **Student Model**: Custom lightweight `LiteCNN` architecture using depthwise separable convolutions and bottleneck layers.
- **Knowledge Distillation**:
  - Trains the student using both **hard labels** (ground truth) and **soft labels** (teacher's output probabilities).
  - Helps the student learn better generalization with fewer parameters.

---

## ğŸ“ Dataset: CREMA-D

- **CREMA-D**: Crowd-sourced Emotional Multimodal Actors Dataset
- **Classes**: `ANG`, `DIS`, `FEA`, `HAP`, `SAD`, `NEU`
- Audio files are preprocessed to 16 kHz and trimmed/padded to 1 second.

> ğŸ’¡ Ensure you download the dataset and set `AUDIO_DIR` in your script accordingly.

---

## ğŸ—ï¸ Model Architecture

### ğŸ§‘â€ğŸ« Teacher: Wav2Vec2

- HuggingFace's `facebook/wav2vec2-base`
- Fine-tuned on emotion classification (6 labels)

### ğŸ§‘â€ğŸ“ Student: LiteCNN

```python
Input: [B, 1, 16000]
â†’ Conv1D + ReLU + MaxPool
â†’ Depthwise + Pointwise Conv
â†’ Bottleneck Conv
â†’ AdaptiveAvgPool
â†’ Flatten â†’ Fully Connected â†’ Output (6 classes)
```

- Lightweight
- Depthwise separable convolutions reduce complexity
- 25xâ€“30x fewer parameters than the teacher

---

## ğŸ§ª Training Strategy

- **Optimizer**: Adam
- **Scheduler**: ReduceLROnPlateau (adaptive learning rate)
- **Loss Function**: Custom Distillation Loss combining:
  - `CrossEntropyLoss` (hard labels)
  - `KL Divergence` (soft labels)
- **Alpha**: Weighting between task and distillation loss
- **Temperature**: Softens logits for distillation

---

## ğŸ“Š Performance Metrics

During training and validation:
- **Accuracy**
- **Precision (macro)**
- **Recall (macro)**
- **Loss Components**: Total, Task, Distillation

---

## ğŸš€ How to Run

### ğŸ”§ 1. Install Dependencies

```bash
pip install torch torchaudio scikit-learn tqdm transformers
```

### ğŸ“¦ 2. Prepare Dataset

- Download the **CREMA-D** dataset.
- Set `AUDIO_DIR` to your local path.

### ğŸ‹ï¸ 3. Train the Student Model

```bash
python train_student_kd.py
```

This script:
- Loads teacher model weights
- Trains student with knowledge distillation
- Logs metrics
- Saves `crema_student_model_kd.pth`

---

## ğŸ’» Deployment

The trained student model is deployed via **Streamlit** for real-time inference.

ğŸ”— **[Launch Streamlit App](https://speechemotiondistill-sjfvberakplyvwznf62bri.streamlit.app/)**  
Upload an audio clip and see the model predict the emotion in seconds.

---

## ğŸ“ˆ Model Compression Summary

| Model         | Parameters | Size     | Accuracy (Val) | Notes                        |
|---------------|------------|----------|----------------|------------------------------|
| Teacher (Wav2Vec2) | ~95M       | Large    | High           | Pre-trained transformer model |
| Student (LiteCNN)  | ~3.5M      | Small    | Slight drop    | Optimized for speed & memory  |

ğŸ§Š **Compression Ratio**: ~27x smaller!

---

## ğŸ§ª Example Outputs

| Audio Input       | Predicted Emotion |
|-------------------|-------------------|
| happy_01.wav      | `HAP`             |
| sad_03.wav        | `SAD`             |
| angry_07.wav      | `ANG`             |

---

## ğŸ™Œ Acknowledgements

- [CREMA-D Dataset](https://github.com/CheyneyComputerScience/CREMA-D)
- [HuggingFace Transformers](https://huggingface.co/transformers/)
- [Streamlit](https://streamlit.io/)

---

## ğŸ“¬ Contact

For questions or improvements, feel free to reach out or open an issue.  
Happy experimenting! ğŸ‰
